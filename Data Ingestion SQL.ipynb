{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load .h5 file and convert it into flat file .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamil\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "C:\\Users\\kamil\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:48: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "h5_filepath = u'C:/Users/kamil/Dropbox (The University of Manchester)/KTP/Work/Data/Original/yanshan_cdu4_2016_Q3_2018.h5'\n",
    "labels_filepath = 'C:/Users/kamil/Dropbox (The University of Manchester)/KTP/Work/Data/Original/labels.csv'\n",
    "csv_filepath = 'C:/Users/kamil/Dropbox (The University of Manchester)/KTP/Work/Data/Original/'\n",
    "range_start = \"2016-07-01 00:00\"\n",
    "range_end = \"2018-12-31 23:59\"\n",
    "dataset1_start_time = '2016-09-01 00:00'\n",
    "dataset1_end_time = '2017-06-01 23:59'\n",
    "dataset2_start_time = '2018-01-01 00:00'\n",
    "dataset2_end_time = '2018-07-01 23:59'\n",
    "\n",
    "list_names = [\"TIC1275\", \"FIC1208\", \"TI1232\", \"TI1225\", \"FI1210\", \"TI1144\", \"TI1226\",\n",
    "              \"PI1212\", \"TIC1201\", \"PI1211\", \"FIC1205\", \"FIQ1337.SUM\", \"TI1317\",\n",
    "              \"TI1231\", \"FIC1206\", \"TI1264\", \"FIC1207\", \"FIC1201\", \"TI1221\", \"FIC1307\",\n",
    "              \"FIQ1334.SUM\", \"TI1316\", \"TI1228\", \"FIC1261\", \"TIC1222\", \"FIC1202\", \"TIC1202\",\n",
    "              \"FIC1304\", \"TI1312\", \"FI1331\", \"TI1229\", \"FIC1264\", \"TIC1265\", \"TIC1223\",\n",
    "              \"FIC1203\", \"TIC1203\", \"FIC1303\", \"FI1332\", \"TI1313\", \"TI1267\", \"FIC1263\",\n",
    "              \"FIC1272\", \"FIQ1271.SUM\", \"TI1276\", \"TI1268\", \"FIC1204\", \"FIC1270\", \"FIC1262\",\n",
    "              \"TIC1204\", \"TI2G2302\", \"TI1230\", \"FIC1209\", \"TIC1269\", \"FIC1265\", \"TI2G2301\", \"FIQ1273.SUM\", \"FIQ2G4504.SUM\"]\n",
    "\n",
    "list_names_corrected = [\"TIC1275\", \"FIC1208\", \"TI1232\", \"TI1225\", \"FI1210\", \"TI1144\", \"TI1226\",\n",
    "                        \"PI1212\", \"TIC1201\", \"PI1211\", \"FIC1205\", \"FIQ1337\", \"TI1317\",\n",
    "                        \"TI1231\", \"FIC1206\", \"TI1264\", \"FIC1207\", \"FIC1201\", \"TI1221\", \"FIC1307\",\n",
    "                        \"FIQ1334\", \"TI1316\", \"TI1228\", \"FIC1261\", \"TIC1222\", \"FIC1202\", \"TIC1202\",\n",
    "                        \"FIC1304\", \"TI1312\", \"FI1331\", \"TI1229\", \"FIC1264\", \"TIC1265\", \"TIC1223\",\n",
    "                        \"FIC1203\", \"TIC1203\", \"FIC1303\", \"FI1332\", \"TI1313\", \"TI1267\", \"FIC1263\",\n",
    "                        \"FIC1272\", \"FIQ1271\", \"TI1276\", \"TI1268\", \"FIC1204\", \"FIC1270\", \"FIC1262\",\n",
    "                        \"TIC1204\", \"TI2G2302\", \"TI1230\", \"FIC1209\", \"TIC1269\", \"FIC1265\", \"TI2G2301\", \"FIQ1273\", \"FIQ2G4504\"]\n",
    "\n",
    "# Load the .h5 file\n",
    "\n",
    "\n",
    "class PHDh5loader():\n",
    "\n",
    "    def __init__(self):\n",
    "        #self.h5File = None\n",
    "        #self.df_data = None\n",
    "        pass\n",
    "\n",
    "    def load(self, h5File):\n",
    "        res = None\n",
    "        with h5py.File(h5File, \"r\") as h5f:\n",
    "            data = h5f.get('/data').value\n",
    "            times = h5f.get('/time').value.astype('datetime64[ns]')\n",
    "            df_data = pd.DataFrame(data=data, index=times)\n",
    "        return df_data\n",
    "\n",
    "\n",
    "h5loader = PHDh5loader()\n",
    "df_data = h5loader.load(h5_filepath)\n",
    "labels = pd.read_csv(labels_filepath)\n",
    "\n",
    "# Assign numbering of measurements to labels\n",
    "iterates = []\n",
    "name_append = []\n",
    "\n",
    "# Assign number from labels to name\n",
    "for i in range(0, len(list_names)):\n",
    "    name = list_names[i]\n",
    "    for j in range(0, len(labels)):\n",
    "        if labels.iloc[j, 1] == name:\n",
    "            iterates.append(j)\n",
    "            name_append.append(name)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# Create dataframe with specified time range and columns\n",
    "data = df_data.loc[range_start:range_end, iterates]\n",
    "data.columns = list_names_corrected\n",
    "\n",
    "# Change the time indices to column, and reset indices from 0\n",
    "data['time'] = data.index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Differentiate summative signal - calculate absolute values at each time point\n",
    "flow_diff = data[['FIQ1334', 'FIQ1271', 'FIQ1273',\n",
    "                  'FIQ1337', 'FIQ2G4504']].diff(periods=60)\n",
    "\n",
    "flow_diff = flow_diff.rename(columns={'FIQ1334': 'FIQ1334_diff', 'FIQ1271': 'FIQ1271_diff', 'FIQ1273': 'FIQ1273_diff',\n",
    "                                      'FIQ1337': 'FIQ1337_diff', 'FIQ2G4504': 'FIQ2G4504_diff'})\n",
    "\n",
    "data = data.drop(['FIQ1334', 'FIQ1271', 'FIQ1273',\n",
    "                  'FIQ1337', 'FIQ2G4504'], axis=1)\n",
    "\n",
    "data = pd.concat([data, flow_diff], axis=1)\n",
    "\n",
    "data = data.fillna(method='bfill')\n",
    "\n",
    "data.to_csv(csv_filepath + 'data.csv')\n",
    "\n",
    "columns_T = np.append([i for i in data.columns if i.startswith(('T'))], 'time')\n",
    "columns_F = np.append([i for i in data.columns if i.startswith(('F'))], 'time')\n",
    "columns_P = np.append([i for i in data.columns if i.startswith(('P'))], 'time')\n",
    "\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "dataset1 = data.loc[(data['time'] >= dataset1_start_time) & (\n",
    "    data['time'] <= dataset1_end_time)].reset_index(drop=True)\n",
    "dataset2 = data.loc[(data['time'] >= dataset2_start_time) & (\n",
    "    data['time'] <= dataset2_end_time)].reset_index(drop=True)\n",
    "\n",
    "dataset1_T = dataset1[columns_T].to_csv(csv_filepath + 'dataset1_T.csv')\n",
    "dataset2_T = dataset2[columns_T].to_csv(csv_filepath + 'dataset2_T.csv')\n",
    "\n",
    "dataset1_F = dataset1[columns_F].to_csv(csv_filepath + 'dataset1_F.csv')\n",
    "dataset2_F = dataset2[columns_F].to_csv(csv_filepath + 'dataset2_F.csv')\n",
    "\n",
    "dataset1_P = dataset1[columns_P].to_csv(csv_filepath + 'dataset1_P.csv')\n",
    "dataset2_P = dataset2[columns_P].to_csv(csv_filepath + 'dataset2_P.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting the flat file onto SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "name = 'data'\n",
    "schema = 'dbo'\n",
    "\n",
    "conn = pyodbc.connect(DRIVER='{SQL Server}',\n",
    "                      SERVER='192.168.1.72, 1433',\n",
    "                      DATABASE='Database_PIL',\n",
    "                      UID='sa',\n",
    "                      PWD='mbdxwko2')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "engine = create_engine(\n",
    "    'mssql+pyodbc://sa:mbdxwko2@192.168.1.72:1433/Database_PIL?driver=ODBC+Driver+17+for+SQL+Server')\n",
    "\n",
    "data.to_sql(name,\n",
    "                       schema=schema,\n",
    "                       con=engine,\n",
    "                       index=False,\n",
    "                       if_exists='replace',\n",
    "                       method='multi',\n",
    "                       chunksize=2097 // data.shape[1]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data on the SQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_tables = 'DROP TABLE dbo.dataset1_F, dbo.dataset1_P, dbo.dataset1_T, dbo.dataset2_F, dbo.dataset2_P, dbo.dataset2_T'\n",
    "\n",
    "create_T_1 = \"SELECT \" + ', '.join(columns_T) + \\\n",
    "    \" INTO dbo.dataset1_T FROM dbo.data WHERE time BETWEEN \" + \\\n",
    "    dataset1_start_time + \" AND \" + dataset1_end_time\n",
    "\n",
    "create_T_1 = \"SELECT \" + ', '.join(columns_T) + \\\n",
    "    \" INTO dbo.dataset1_T FROM dbo.data WHERE time BETWEEN \" + \\\n",
    "    dataset2_start_time + \" AND \" + dataset2_end_time\n",
    "\n",
    "create_F_1 = \"SELECT \" + ', '.join(columns_F) + \\\n",
    "    \" INTO dbo.dataset1_T FROM dbo.data WHERE time BETWEEN \" + \\\n",
    "    dataset1_start_time + \" AND \" + dataset1_end_time\n",
    "\n",
    "create_F_1 = \"SELECT \" + ', '.join(columns_F) + \\\n",
    "    \" INTO dbo.dataset1_T FROM dbo.data WHERE time BETWEEN \" + \\\n",
    "    dataset2_start_time + \" AND \" + dataset2_end_time\n",
    "\n",
    "create_P_1 = \"SELECT \" + ', '.join(columns_P) + \\\n",
    "    \" INTO dbo.dataset1_T FROM dbo.data WHERE time BETWEEN \" + \\\n",
    "    dataset1_start_time + \" AND \" + dataset1_end_time\n",
    "\n",
    "create_P_1 = \"SELECT \" + ', '.join(columns_P) + \\\n",
    "    \" INTO dbo.dataset1_T FROM dbo.data WHERE time BETWEEN \" + \\\n",
    "    dataset2_start_time + \" AND \" + dataset2_end_time\n",
    "\n",
    "queries = np.array([drop_tables, create_T_1, create_T_2,\n",
    "                    create_F_1, create_F_2, create_P_1, create_P_2])\n",
    "\n",
    "for query in queries:\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        cursor.commit()\n",
    "    except:\n",
    "        cursor.commit()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
